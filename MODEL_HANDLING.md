# Model Handling & Storage Guide

## âœ… Models are EXCLUDED from Git

Your `.gitignore` is configured to **automatically exclude all models** from being uploaded to GitHub. This saves storage and speeds up deployment.

---

## ğŸš« What Gets EXCLUDED (Not Uploaded)

These are automatically ignored by Git:

### Large Model Files:
- âœ… `models/` - Model directory
- âœ… `gpt2/`, `gpt2-medium/`, `gpt2-large/`, `gpt2-xl/`
- âœ… `distilbert*/`, `roberta*/`, `bert*/`
- âœ… `t5*/`, `flan*/`, `gemma*/`, `llama*/`, `mistral*/`
- âœ… `.cache/` - HuggingFace cache

### Model File Types:
- âœ… `*.bin` - PyTorch binary files (largest files!)
- âœ… `*.safetensors` - SafeTensors format
- âœ… `*.gguf` - GGUF format (llama.cpp)
- âœ… `*.h5` - Keras/TensorFlow weights
- âœ… `*.onnx` - ONNX format
- âœ… `*.pb` - TensorFlow protobuf
- âœ… `*.msgpack` - MessagePack files

### Runtime Data:
- âœ… `uploads/` - User PDFs
- âœ… `data/` - Processed chunks & embeddings
- âœ… `logs/` - Log files
- âœ… `*.pkl`, `*.faiss` - Index files
- âœ… `venv/` - Virtual environment

---

## âœ… What Gets UPLOADED (Small Files Only)

Only your **code** goes to GitHub:

```
âœ… app.py                    - Flask application
âœ… config.py                 - Configuration
âœ… qa_engine.py              - QA logic
âœ… pdf_processor.py          - PDF processing
âœ… requirements.txt          - Dependencies list
âœ… start_app.sh              - Startup script
âœ… templates/                - HTML templates
âœ… static/                   - CSS/JS files
âœ… README.md                 - Documentation
```

**Total size**: ~100KB (very small!)

---

## ğŸ“¥ How Models Get Downloaded

Models are downloaded **automatically on the server** when the app first runs:

### On First Run:

```bash
# When you run on the server
./start_app.sh

# The app automatically downloads models from HuggingFace:
# 1. Sentence transformer: "all-MiniLM-L6-v2" (~90MB)
# 2. QA model (if enabled): "distilbert-base-cased-distilled-squad" (~260MB)
# 3. Generator (if enabled): "gpt2" or custom model
```

### Where Models are Stored on Server:

```
~/.cache/huggingface/        # HuggingFace cache (auto-created)
~/pdf-qa-system/models/      # Local models (if any)
```

---

## ğŸ” Verify What Will Be Uploaded

Before pushing to GitHub, check what Git will upload:

```bash
# See what will be committed
git status

# See ignored files (should include models)
git status --ignored

# Check file sizes that would be uploaded
git ls-files | xargs ls -lh
```

If you see large files (>10MB), they should NOT be uploaded!

---

## ğŸ›¡ï¸ Model Download Strategy

### Current Setup (Recommended):

**Your config.py:**
```python
GENERATOR_CONFIG = {
    'model_name': 'none',      # No large generator model
    'use_generator': False,
}

EMBEDDING_CONFIG = {
    'model_name': 'all-MiniLM-L6-v2',  # Small embedding model (~90MB)
}
```

**Download on server:**
- âœ… Embedding model: Auto-downloaded (~90MB, one-time)
- âœ… Fast download: ~30 seconds on good connection
- âœ… Cached: Only downloads once

---

## ğŸš€ Deployment Workflow

### Step 1: Push Code Only (Windows)

```bash
git add .
git commit -m "Update application"
git push
```

**What's uploaded**: Only code files (~100KB)
**What's NOT uploaded**: Models, data, logs (excluded by .gitignore)

### Step 2: Models Download on Server

```bash
# On server
git pull
./start_app.sh
```

**First time:**
- âœ… Creates virtual environment
- âœ… Installs Python packages
- âœ… Downloads models from HuggingFace (~90MB)
- â±ï¸ Takes 2-5 minutes (one-time setup)

**Subsequent updates:**
- âœ… Just pulls code changes
- âœ… Models already cached
- â±ï¸ Takes 5-10 seconds

---

## ğŸ“Š Storage Requirements

### GitHub Repository:
```
Code only: ~100KB
âœ… Very small, fast uploads
```

### Server Storage:
```
Application code:        ~100KB
Dependencies (venv):     ~500MB
Models (cache):          ~90MB  (embedding model)
Runtime data:            Variable (depends on usage)
---
Total initial:           ~600MB
```

### With Optional Large Models:
```
GPT-2 Medium:            +1.5GB
GPT-2 Large:             +3GB
T5-Large:                +3GB
---
Only install if needed!
```

---

## âš ï¸ Troubleshooting

### If Models Were Accidentally Added:

```bash
# Remove from Git tracking (keeps local file)
git rm --cached -r models/
git rm --cached -r gpt2-medium/
git rm --cached *.bin
git rm --cached *.safetensors

# Commit the removal
git commit -m "Remove large model files from Git"

# Push
git push
```

### If .gitignore Wasn't Working:

```bash
# Clear Git cache and re-add files
git rm -r --cached .
git add .
git commit -m "Apply .gitignore rules"
git push
```

### If Download Fails on Server:

```bash
# Manually download specific model
python3 -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"

# Or set cache directory
export HF_HOME=/path/to/cache
./start_app.sh
```

---

## ğŸ¯ Best Practices

### âœ… DO:
- Use small, efficient models (all-MiniLM-L6-v2)
- Let HuggingFace handle model downloads
- Keep models in cache directory
- Use `.gitignore` to exclude models

### âŒ DON'T:
- Commit `.bin`, `.safetensors` files to Git
- Upload models to GitHub
- Store models in project directory
- Use huge models unless necessary

---

## ğŸ“ Model Configuration Options

### Option 1: No Generator (Fastest, Smallest)
```python
GENERATOR_CONFIG = {
    'model_name': 'none',
    'use_generator': False,
}
```
**Size**: ~90MB total
**Speed**: Fast
**Accuracy**: Good (extractive answers)

### Option 2: With Small Generator
```python
GENERATOR_CONFIG = {
    'model_name': 'gpt2',  # 500MB
    'use_generator': True,
}
```
**Size**: ~600MB total
**Speed**: Medium
**Accuracy**: Better (generative answers)

### Option 3: With Advanced QA
```python
QA_CONFIG = {
    'use_advanced_qa': True,
    'advanced_qa_model': 'distilbert-base-cased-distilled-squad',  # 260MB
}
```
**Size**: ~350MB total
**Speed**: Medium-Fast
**Accuracy**: Better (BERT-based QA)

---

## ğŸ”„ Summary

| Item | Uploaded to Git? | Downloaded on Server? |
|------|------------------|----------------------|
| **Code files** | âœ… Yes (~100KB) | âœ… Yes (via git pull) |
| **Models** | âŒ No (excluded) | âœ… Yes (auto-download) |
| **Dependencies** | ğŸ“ List only (requirements.txt) | âœ… Yes (pip install) |
| **User data** | âŒ No (excluded) | ğŸ”„ Created at runtime |

**Result**: Fast uploads, efficient deployment, no storage waste! âœ¨

---

## ğŸ‰ You're All Set!

Your `.gitignore` is properly configured to:
- âœ… Exclude all model files
- âœ… Exclude runtime data
- âœ… Keep repository small
- âœ… Enable fast deployments

Just use `git push` and models will download automatically on the server!
