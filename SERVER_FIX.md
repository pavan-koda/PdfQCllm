# Server Fix Guide - Gemma Gated Model Error

## âŒ Problem

Your server's `config.py` is trying to load `google/gemma-7b-it` which requires HuggingFace authentication. The error message shows:

```
ERROR: You are trying to access a gated repo.
Access to model google/gemma-7b-it is restricted.
```

## âœ… Solution - Replace config.py on Server

### **Option 1: Push Updated Config via Git (Recommended)**

On your Windows machine:
```bash
cd d:/A/LLM/GPT2\(M\)/PDF-QA-System

# Add and commit the fixed config
git add config.py
git commit -m "Fix config: use simple models instead of Gemma"
git push
```

On your server:
```bash
cd ~/PDF-QA-System

# Pull the updated config
git pull

# Restart the app
./start_app.sh
```

### **Option 2: Manually Update config.py on Server**

SSH to server and edit:
```bash
cd ~/PDF-QA-System
nano config.py
```

Replace the **entire file** with this:

```python
"""
Configuration file for the PDF Q&A System
Auto-generated by model_selector.py
"""

# QA Engine Configuration
QA_CONFIG = {
    # Mode: 'extractive' (fast, accurate, uses regex) or 'generative' (slower, uses AI)
    'mode': 'extractive' if 'extractive' == 'extractive' else 'advanced',

    # Use advanced QA model (requires more memory but better accuracy)
    'use_advanced_qa': False,

    # Advanced QA model to use if use_advanced_qa=True
    'advanced_qa_model': 'extractive',

    # Use full document context instead of just top chunks (better accuracy)
    'use_full_context': True,

    # Number of top chunks to retrieve (only used if use_full_context=False)
    'top_k_chunks': 10,

    # Maximum answer length
    'max_answer_length': 1200,
}

# Embedding Model Configuration
EMBEDDING_CONFIG = {
    'model_name': 'all-MiniLM-L6-v2',
}

# Generator Model Configuration
GENERATOR_CONFIG = {
    'model_name': 'none',
    'use_generator': False,
}

# PDF Processing Configuration
PDF_CONFIG = {
    'chunk_size': 400,  # words per chunk
    'chunk_overlap': 50,  # overlapping words between chunks
}

# Flask Configuration
FLASK_CONFIG = {
    'host': '0.0.0.0',
    'port': 5000,
    'debug': True,
    'max_content_length': 16 * 1024 * 1024,  # 16MB
}
```

Save with `Ctrl+X`, then `Y`, then `Enter`.

### **Option 3: Delete config.py and Reconfigure**

On server:
```bash
cd ~/PDF-QA-System

# Delete old config
rm config.py

# Run script - it will prompt for model selection
./start_app.sh

# When prompted, select:
# - Embedding model: all-MiniLM-L6-v2
# - QA mode: Extractive (simple, fast)
# - Generator: None
```

---

## ğŸ“‹ What the Fixed Config Does

### **Current (Broken) Server Config:**
- âŒ Embedding: `BAAI/bge-base-en-v1.5`
- âŒ QA Model: `bert-large-uncased-whole-word-masking-finetuned-squad`
- âŒ Generator: `google/gemma-7b-it` (GATED - requires auth)

### **New (Working) Config:**
- âœ… Embedding: `all-MiniLM-L6-v2` (~90MB)
- âœ… QA Mode: Extractive (regex-based, no model needed)
- âœ… Generator: None (not needed for extractive QA)
- âœ… Full Context: Enabled (reads entire PDF)

**Benefits:**
- No authentication needed
- Smaller download (~90MB instead of ~7GB)
- Faster startup
- Better accuracy with full context mode

---

## ğŸš€ Quick Commands

```bash
# On Windows - Update and push
cd d:/A/LLM/GPT2\(M\)/PDF-QA-System
git add config.py
git commit -m "Fix config for server"
git push

# On Server - Pull and run
cd ~/PDF-QA-System
git pull
./start_app.sh
```

---

## âœ… Expected Output After Fix

```
========================================================================
                    PDF Q&A SYSTEM - STARTUP
========================================================================

[INFO] Existing configuration found

Do you want to reconfigure models? [y/n]: n
[INFO] Using existing model configuration

[INFO] Checking for required dependencies...

========================================================================
                    STARTING APPLICATION
========================================================================

Starting Flask server...
INFO:qa_engine:Using device: cpu
INFO:qa_engine:Loading sentence transformer model...
INFO:qa_engine:Sentence transformer loaded successfully
INFO:qa_engine:Skipping generator model (not needed for extractive mode)
 * Serving Flask app 'app'
 * Running on http://0.0.0.0:5000

Once started, open your browser to:
  > http://localhost:5000
  > http://127.0.0.1:5000

Press Ctrl+C to stop the server
```

---

## ğŸ”§ Alternative: Use Gemma with Authentication

If you really want to use Gemma:

```bash
# On server
huggingface-cli login
# Paste your HuggingFace token

# Accept license at:
# https://huggingface.co/google/gemma-7b-it

# Then run app
./start_app.sh
```

**Warning:** Gemma-7b-it is ~14GB and very slow on CPU. Not recommended unless you have GPU.

---

## ğŸ“ Summary

Your server has an old config.py file with Gemma configured. Update it using Option 1 (Git) or Option 2 (manual edit), then restart. The app will work perfectly with the lightweight models! ğŸ‰
