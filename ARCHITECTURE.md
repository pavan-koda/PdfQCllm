# ğŸ—ï¸ System Architecture - Vision PDF QA

Detailed technical architecture of the vision-based PDF QA system.

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE                            â”‚
â”‚                     (Flask Web App)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                 â”‚
             â”‚ Upload PDF                      â”‚ Ask Question
             â†“                                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VISION PDF PROCESSOR     â”‚    â”‚    VISION QA ENGINE          â”‚
â”‚   (vision_pdf_processor)   â”‚    â”‚   (vision_qa_engine)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                 â”‚
             â”‚ Process PDF                     â”‚ Retrieve + Answer
             â†“                                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PyMuPDF (fitz)          â”‚    â”‚   COLPALI RETRIEVER          â”‚
â”‚   - Render pages â†’ images  â”‚    â”‚   (colpali_retriever)        â”‚
â”‚   - Extract text           â”‚    â”‚   - Visual similarity search â”‚
â”‚   - Extract embedded imgs  â”‚    â”‚   - FAISS index              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                 â”‚
             â”‚ Store                           â”‚ Query
             â†“                                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CHROMADB                                  â”‚
â”‚              (Multimodal Vector Database)                         â”‚
â”‚   - Page images metadata                                          â”‚
â”‚   - Text embeddings                                               â”‚
â”‚   - Visual embeddings                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ Page images + context
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLAMA 3.2-VISION (via Ollama)                  â”‚
â”‚              - Multimodal understanding                           â”‚
â”‚              - Image + Text analysis                              â”‚
â”‚              - Answer generation                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Component Details

### 1. Vision PDF Processor (`vision_pdf_processor.py`)

**Purpose:** Convert PDFs to processable format for vision models

**Key Features:**
- Renders PDF pages to images (150 DPI default)
- Extracts text using PyMuPDF's `get_text()`
- Extracts embedded images separately
- Processes in batches for memory efficiency

**Technology:**
- **PyMuPDF (fitz):** Superior PDF rendering and extraction
- **Pillow:** Image format conversion
- **Base64:** Encoding for API transmission

**Process Flow:**
```python
PDF File
    â†“
Open with PyMuPDF
    â†“
For each page:
    â”œâ”€â†’ Render to PNG (150 DPI)
    â”œâ”€â†’ Extract text content
    â””â”€â†’ Extract embedded images
    â†“
Save to session directory:
    â”œâ”€â†’ page_0001.png, page_0002.png, ...
    â”œâ”€â†’ embedded_images/page_0001_img_001.png, ...
    â””â”€â†’ Metadata (pages, text, images)
```

**Performance:**
- **Speed:** 2-5 seconds per page
- **Memory:** ~50MB per page at 150 DPI
- **Optimization:** Batch processing reduces overhead

### 2. ColPali Retriever (`colpali_retriever.py`)

**Purpose:** Visual similarity search across document pages

**How It Works:**
1. **Indexing:**
   - Encodes each page image into vector embedding
   - Creates FAISS index for fast similarity search
   - Stores embeddings + metadata

2. **Retrieval:**
   - Encodes user query to vector
   - Searches FAISS index for similar pages
   - Returns top-k most relevant pages

**Technology:**
- **ColPali:** Vision-language retrieval model
- **CLIP (fallback):** OpenAI's vision-text model
- **FAISS:** Facebook's similarity search library

**Key Advantages:**
- âœ… Understands visual layout (diagrams, charts)
- âœ… Finds pages even without text
- âœ… Fast: Searches 500 pages in <1 second
- âœ… Works with unstructured documents

**Architecture:**
```
Page Images â†’ ColPali Encoder â†’ Visual Embeddings â†’ FAISS Index

Query Text â†’ ColPali Encoder â†’ Query Embedding â†’ Search FAISS
                                                      â†“
                                              Top-k Page Indices
```

### 3. Vision QA Engine (`vision_qa_engine.py`)

**Purpose:** Answer questions using multimodal understanding

**Components:**

#### A. ChromaDB Integration
```python
Collection per session:
â”œâ”€â†’ Documents: Page text content
â”œâ”€â†’ Metadatas: {page, image_path, has_text, ...}
â”œâ”€â†’ Embeddings: Text embeddings (auto-generated)
â””â”€â†’ IDs: page_1, page_2, ..., page_N
```

#### B. Ollama Client
```python
Query Process:
1. Receive question
2. Retrieve relevant pages (ColPali or ChromaDB)
3. Load page image
4. Encode image to base64
5. Send to Ollama API:
   {
     "model": "llama3.2-vision:11b",
     "prompt": question + context,
     "images": [base64_image]
   }
6. Parse response
7. Return answer
```

#### C. Multi-Strategy Retrieval
```python
Strategy 1: ColPali Visual Search (preferred)
    - Best for diagrams, charts, visual content
    - Fast and accurate

Strategy 2: ChromaDB Text Search (fallback)
    - Good for text-heavy documents
    - Used when ColPali unavailable
```

**Answer Generation Flow:**
```
User Question
    â†“
Retrieve Pages (ColPali or ChromaDB)
    â†“
Get top 5 relevant pages
    â†“
For best page:
    â”œâ”€â†’ Load page image
    â”œâ”€â†’ Get text context from top 3 pages
    â””â”€â†’ Send to Llama 3.2-Vision
    â†“
Llama analyzes:
    â”œâ”€â†’ Visual content (diagrams, charts, images)
    â”œâ”€â†’ Text content (words, paragraphs)
    â””â”€â†’ Layout (tables, structure)
    â†“
Generate comprehensive answer
    â†“
Return to user (5-15 seconds total)
```

### 4. Llama 3.2-Vision (via Ollama)

**Purpose:** Multimodal AI for vision + text understanding

**Model Specs:**
- **Parameters:** 11B (recommended) or 90B (best quality)
- **Context Window:** 8,192 tokens
- **Vision Capabilities:**
  - Object detection
  - OCR (text in images)
  - Chart/diagram understanding
  - Scene description
  - Visual reasoning

**Ollama Integration:**
```bash
# Local inference server
ollama serve â†’ localhost:11434

# API endpoint
POST /api/generate
{
  "model": "llama3.2-vision:11b",
  "prompt": "Explain this diagram",
  "images": ["base64_encoded_image"],
  "options": {
    "num_predict": 800,
    "temperature": 0.7
  }
}

# Response
{
  "response": "The diagram shows...",
  "total_duration": 12345678900
}
```

**Advantages of Ollama:**
- âœ… 100% local (no cloud)
- âœ… No API keys needed
- âœ… Fast inference (GPU optional)
- âœ… Easy model management
- âœ… Cross-platform (Windows, macOS, Linux)

## ğŸ’¾ Data Flow

### Upload Process

```
1. User uploads PDF
   â””â”€â†’ Save to uploads/{session_id}_{filename}.pdf

2. Vision PDF Processor
   â”œâ”€â†’ Open PDF with PyMuPDF
   â”œâ”€â†’ Process each page:
   â”‚   â”œâ”€â†’ Render to PNG (150 DPI)
   â”‚   â”œâ”€â†’ Extract text
   â”‚   â””â”€â†’ Extract images
   â””â”€â†’ Save to processed_pdfs/{session_id}/
       â”œâ”€â†’ page_0001.png
       â”œâ”€â†’ page_0002.png
       â””â”€â†’ embedded_images/

3. ColPali Indexing
   â”œâ”€â†’ Encode all page images
   â”œâ”€â†’ Create FAISS index
   â””â”€â†’ Save to data/{session_id}_colpali.faiss

4. ChromaDB Storage
   â”œâ”€â†’ Create collection pdf_{session_id}
   â”œâ”€â†’ Add documents (page texts)
   â”œâ”€â†’ Add metadatas (image paths, page numbers)
   â””â”€â†’ Auto-generate embeddings

5. Ready for queries! âœ…
```

### Query Process

```
1. User asks question
   â””â”€â†’ POST /ask {question: "..."}

2. Retrieval Phase
   â”œâ”€â†’ ColPali search (visual)
   â”‚   â”œâ”€â†’ Encode query
   â”‚   â”œâ”€â†’ Search FAISS index
   â”‚   â””â”€â†’ Get top-k pages
   â”‚
   â””â”€â†’ ChromaDB search (text fallback)
       â”œâ”€â†’ Embed query
       â”œâ”€â†’ Search collection
       â””â”€â†’ Get top-k documents

3. Context Building
   â”œâ”€â†’ Best page image
   â”œâ”€â†’ Text from top 3 pages
   â””â”€â†’ Combine into context

4. Vision AI Analysis
   â”œâ”€â†’ Encode page image to base64
   â”œâ”€â†’ Build prompt (question + context)
   â”œâ”€â†’ Send to Ollama
   â””â”€â†’ Receive answer

5. Response
   â””â”€â†’ Return {answer, response_time, page_used}
```

## ğŸ—„ï¸ Storage Structure

```
PDF-QA-System/
â”œâ”€â”€ uploads/
â”‚   â””â”€â”€ {session_id}_{filename}.pdf
â”‚
â”œâ”€â”€ processed_pdfs/
â”‚   â””â”€â”€ {session_id}/
â”‚       â”œâ”€â”€ page_0001.png
â”‚       â”œâ”€â”€ page_0002.png
â”‚       â”œâ”€â”€ page_0003.png
â”‚       â””â”€â”€ embedded_images/
â”‚           â”œâ”€â”€ page_0001_img_001.png
â”‚           â””â”€â”€ page_0002_img_001.png
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ {session_id}_colpali.faiss       # FAISS index
â”‚   â””â”€â”€ {session_id}_colpali_meta.pkl    # Metadata
â”‚
â”œâ”€â”€ chroma_db/
â”‚   â””â”€â”€ {chroma_internal_structure}      # ChromaDB data
â”‚
â””â”€â”€ logs/
    â””â”€â”€ vision_performance.txt           # Query logs
```

## âš™ï¸ Configuration Options

### PDF Processing

```python
# vision_pdf_processor.py
VisionPDFProcessor(
    dpi=150,              # Image quality (72-300)
    extract_images=True,  # Extract embedded images
    extract_text=True,    # Extract text content
    batch_size=10         # Pages per batch
)
```

### ColPali Retrieval

```python
# colpali_retriever.py
ColPaliRetriever(
    model_name="vidore/colpali",  # Model to use
    device="cuda",                # GPU/CPU
    use_half_precision=True       # FP16 for speed
)
```

### Vision QA

```python
# vision_qa_engine.py
VisionQAEngine(
    ollama_url="http://localhost:11434",
    model_name="llama3.2-vision:11b",
    chroma_persist_dir="chroma_db",
    use_colpali=True
)
```

## ğŸ” Security Considerations

### Input Validation
- PDF file type checking
- File size limits (500MB)
- Question length limits (1000 chars)
- Session ID validation

### Session Isolation
- Unique session ID per upload
- Separate storage directories
- Collection namespacing in ChromaDB

### Data Cleanup
- Automatic cleanup on session reset
- Temporary file removal
- Database collection deletion

### Production Deployment
- Use Gunicorn instead of Flask dev server
- Add authentication (Flask-Login)
- Enable HTTPS (nginx reverse proxy)
- Rate limiting
- Input sanitization

## ğŸ“ˆ Performance Optimization

### CPU-Bound Operations
- PDF rendering (PyMuPDF)
- Image encoding (Pillow)
- FAISS indexing

**Optimization:**
- Batch processing
- Lower DPI for faster rendering
- Lazy loading

### GPU-Bound Operations
- ColPali encoding
- Llama 3.2-Vision inference

**Optimization:**
- Use CUDA PyTorch
- FP16 precision
- Batch inference

### Memory Management
- Process PDFs in chunks
- Release unused page data
- Clear embeddings after indexing

### Caching Strategy
- Cache embeddings (FAISS + ChromaDB)
- Cache page images on disk
- No caching of answers (always fresh)

## ğŸ”® Future Enhancements

1. **OCR Integration**
   - Add Tesseract for scanned PDFs
   - Combine OCR + native text

2. **Multi-PDF Support**
   - Search across multiple documents
   - Cross-document queries

3. **Advanced Retrieval**
   - Hybrid search (dense + sparse)
   - Re-ranking models
   - Query expansion

4. **Better Vision Models**
   - LLaVA integration
   - GPT-4V API option
   - CogVLM support

5. **Export Features**
   - Save Q&A to PDF
   - Export highlights
   - Bookmark important pages

---

**Architecture designed for:**
- âœ… Scalability (500+ page PDFs)
- âœ… Accuracy (vision + text understanding)
- âœ… Speed (ColPali fast retrieval)
- âœ… Privacy (100% local processing)
- âœ… Flexibility (modular components)
